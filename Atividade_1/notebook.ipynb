{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dados\n",
        "Os arquivos estão comprimidos em \".zip\", portanto temos que extrair os arquivos do corpus para outra pasta que será chamada **\"corpus_extracted\"**.\n"
      ],
      "metadata": {
        "id": "B3t8dn3_f4to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_file_path = 'corpus.zip'\n",
        "extract_to_path = 'corpus_extracted'\n",
        "os.makedirs(extract_to_path, exist_ok=True) # extraindo os arquivos utilizando a biblioteca nativa do python zipfile\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "print(f'Arquivos extraídos para {extract_to_path}')\n",
        "\n",
        "print(f'Número de arquivos extraídos: {len(os.listdir(\"corpus_extracted\"))}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XA0-3GLgH-h",
        "outputId": "fea3e078-8f7d-4748-cf1b-10aa9cac27fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos extraídos para corpus_extracted\n",
            "Número de arquivos extraídos: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após isso temos que importar os arquivos do corpus \".json\" e colocá-lo em um só\n",
        "arquivo de texto em formato \".txt\".\n",
        "O nome do arquivo \".txt\" com o compilado de todos os textos do corpus irá se chamar **\"sum_corpus.txt**\"."
      ],
      "metadata": {
        "id": "qytpKWd4pL7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import read_files\n",
        "import os\n",
        "\n",
        "\n",
        "if not os.path.exists('sum_corpus.txt'):\n",
        "  for each in os.listdir(\"corpus_extracted\"):\n",
        "      data_json = read_files.read_json(\"corpus_extracted/\" + each)\n",
        "      with open(\"sum_corpus.txt\", \"a\", encoding=\"utf-8\") as file:\n",
        "          file.write(data_json[\"text\"])\n",
        "  print(f\"Arquivo de texto sum_corpus.txt criado\")"
      ],
      "metadata": {
        "id": "71PTeKh4pPBz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE (Byte Pair Encoding)\n",
        "Agora o algoritmo BPE pode ser aplicado no arquivo **\"sum_corpus.txt\"** para fazer os *merges* e assim criar o vocabulário de *tokens*."
      ],
      "metadata": {
        "id": "GosVB62WqiTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bpe\n",
        "\n",
        "\n",
        "with open(\"sum_corpus.txt\", \"r\", encoding=\"utf-8\") as file: # abre o arquivo sum_corpus.txt e lê o seu conteúdo\n",
        "    string = file.read()\n",
        "string = list(map(int, string.encode('utf-8'))) # faz o encode do conteúdo lido para utf-8\n",
        "num_merges = 20 # número de merges que será feito no algoritmo bpe\n",
        "merges = bpe.merge_loop(num_merges, string) # chama a função merge_loop que faz os merges\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)} # inicializa o vocabulário com os 256 tokens iniciais.\n",
        "for (p0, p1), idx in merges.items():\n",
        "    if vocab[p0] + vocab[p1] not in vocab.values():\n",
        "        vocab[idx] = vocab[p0] + vocab[p1]  # aqui os merges são representados e inseridos no vocabulário\n",
        "print(vocab)  # vocabulário resultante com os merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1reV2RL5Ahd",
        "outputId": "8e067cb2-1f8c-4122-c4da-ec418381d057"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (111, 32) into a new token 256\n",
            "merging (97, 32) into a new token 257\n",
            "merging (101, 32) into a new token 258\n",
            "merging (115, 32) into a new token 259\n",
            "merging (44, 32) into a new token 260\n",
            "merging (100, 258) into a new token 261\n",
            "merging (101, 110) into a new token 262\n",
            "merging (109, 32) into a new token 263\n",
            "merging (111, 114) into a new token 264\n",
            "merging (101, 114) into a new token 265\n",
            "merging (97, 110) into a new token 266\n",
            "merging (97, 114) into a new token 267\n",
            "merging (101, 115) into a new token 268\n",
            "merging (99, 111) into a new token 269\n",
            "merging (46, 32) into a new token 270\n",
            "merging (100, 256) into a new token 271\n",
            "merging (111, 259) into a new token 272\n",
            "merging (105, 110) into a new token 273\n",
            "merging (97, 108) into a new token 274\n",
            "merging (97, 259) into a new token 275\n",
            "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'o ', 257: b'a ', 258: b'e ', 259: b's ', 260: b', ', 261: b'de ', 262: b'en', 263: b'm ', 264: b'or', 265: b'er', 266: b'an', 267: b'ar', 268: b'es', 269: b'co', 270: b'. ', 271: b'do ', 272: b'os ', 273: b'in', 274: b'al', 275: b'as '}\n"
          ]
        }
      ]
    }
  ]
}