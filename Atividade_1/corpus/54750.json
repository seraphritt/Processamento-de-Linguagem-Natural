{
 "id": "54750",
 "text": "Em análise numérica, o método de Newton (ou Método de Newton–Raphson), desenvolvido por Isaac Newton e Joseph Raphson, tem o objetivo de estimar as raízes de uma função. Para isso, escolhe-se uma aproximação inicial para esta. Após isso, calcula-se a equação da reta tangente (por meio da derivada) ao gráfico da função nesse ponto e a interseção dela com o eixo das abcissas, a fim de encontrar uma melhor aproximação para a raiz. Repetindo-se o processo, cria-se um método iterativo para encontrarmos a raiz da função. Em notação matemática, o método de Newton é dado pela seguinte sequência recursiva: x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} , n\\in \\mathbb{N} onde x_0 é uma aproximação inicial dada, n indica a n-ésima iteração do algoritmo e f'(x_n) é a derivada da função f no ponto x_n. == Interpretação geométrica == Consideremos o problema de calcular a raiz de uma função f, conforme a figura ao lado. Queremos calcular x_1 em função de x_0, sabendo que x_1 será a cota no eixo das abcissas interceptado pela reta tangente à curva, originada por x_0. A equação da reta tangente ao gráfico da função f no ponto (x_0, f(x_0)) tem inclinação m=f'(x_0) e é dada por y - f(x_0) = f'(x_0)(x - x_0) Sabendo que essa reta passa por (x_1, 0), temos que 0 - f(x_0) = f'(x_0)(x_1 - x_0). Portanto, x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}. De modo geral, temos x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}. == Análise de convergência == Devemos ter em mente que, mesmo se a condição estabelecida na introdução for satisfeita, o método de Newton poderá não convergir para a raiz. Seja f(x) uma função e sua derivada diferente de zero, definimos uma função \\phi(x) como: \\phi(x) = x - \\frac{f(x)}{f'(x)} Consideramos x* uma aproximação da solução x de f(x)=0 tal que f'(x*)≠0 e |x – x*| seja “pequeno”. Expandimos \\phi(x) por Série de Taylor em torno de x* e obtemos: \\phi(x) = \\phi(x^*) + (x - x^*)\\phi'(x^*) + \\frac{(x - x^*)^2}{2}\\phi(x^*) + O((x - x^*)^3) Para a dedução do método de Newton, vamos supor que |x - x*| é pequeno, logo, o termo (x - x*)² será muito menor. Com isso, dizemos que: \\phi(x) \\approx \\phi(x^*) + (x - x^*)\\phi'(x^*) Pelo processo iterativo do método do ponto fixo, sabemos que: \\phi(x^*) = x^* - \\frac{f(x^*)}{f'(x^*)} = x^* \\phi'(x^*) = 1 - \\frac{ f'(x^*) f'(x^*) - f(x^*)f(x^*) }{(f'(x))^2} = 1 - 1 = 0 \\phi(x^*) = \\frac{(f'(x^*)f(x^*) + f(x^*)f(x^*))(f'(x^*))^2 - 2 f(x^*)f(x^*)f'(x^*)}{(f'(x^*))^4} = \\frac{f(x^*)}{f'(x^*)} Portanto: \\phi(x) = x^* + (x - x^*)^2\\frac{\\phi(x^*)}{2} + O((x - x^*)^3) \\phi(x) \\approx x^* + (x - x^*)^2\\frac{\\phi(x^*)}{2} Logo: x_{n+1} = \\phi(x_n) x^* + (x_n - x^*)^2\\frac{\\phi(x^*)}{2} (x_{n+1} - x^*) \\approx (x_n - x^*)^2\\frac{\\phi(x^*)}{2} Considerando (xn \\- x*) o erro absoluto, obtemos: \\epsilon_{n+1} \\approx \\epsilon^2_n\\frac{\\phi(x^*)}{2} = \\frac {1}{2} |\\frac {f(x^*)}{f'(x^*)}|\\epsilon_{n}^2 Com isso, observamos que o erro (\\epsilon_n) é de ordem quadrática e, por isso, a iteração convergirá rapidamente para a raiz da função. == Generalização == O método de Newton é uma poderosa ferramenta para resolvermos equações de uma variável (f(x)=0). Esse método, contudo, pode ser utilizado em problemas mais complexos, como na solução de equações do tipo Ax=b, em que x e b são vetores e A é uma matriz. Queremos, portanto, generalizar o método de Newton para resolvermos um sistema de equações da forma: \\begin{array}{rcl} f_{1}(x_{1},x_{2},...,x_{n})&=&0\\\\\\ f_{2}(x_{1},x_{2},...,x_{n})&=&0\\\\\\ f_{3}(x_{1},x_{2},...,x_{n})&=&0\\\\\\ &\\vdots&\\\\\\ f_{n}(x_{1},x_{2},...,x_{n})&=&0 \\end{array} Podemos analisar esse sistema de equações na forma vetorial, definindo o vetor F(x) tal que: F(x) = \\begin{bmatrix} f_{1}(x_{1},x_{2},...,x_{n}) \\\\\\ f_{2}(x_{1},x_{2},...,x_{n}) \\\\\\ \\vdots \\\\\\ f_{n}(x_{1},x_{2},...,x_{n}) \\end{bmatrix} Para resolvermos o problema de uma variável (f(x)=0), nós expandíamos a função f(x) em torno de x* por sua Série de Taylor, de modo a obtermos: f(x)\\approx f(x^*) + (x-x^*)f'(x^*) , sendo x* uma aproximação para a solução de f(x)=0 . De modo equivalente, o problema matricial se resume a resolver a equação F(x)=0, e devemos expandir a função F(x) em torno de x*, sendo x* uma aproximação para a solução de F(x)=0. Efetuando-se essa expansão, obteremos: F(x)\\approx F(x^*) + (x-x^*)F'(x^*) Portanto, será necessário definirmos a derivada de F(x). Definimos, então, a Matriz Jacobiana por: J_F=\\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\\ \\vdots & \\ddots & \\vdots \\\\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} E percebemos que a Matriz Jacobiana, ou o Jacobiano do vetor F(x), é a matriz formada pelas derivadas parciais das componentes de F(x): F'(x) = (J_{F})_{ij} = \\frac{\\partial f_i}{\\partial x_j} Logo, podemos reescrever a expansão por Série de Taylor de F(x) como F(x)\\approx F(x^*) + (x-x^*)J_F(x^*) . Também de acordo com o problema de uma variável, tínhamos que o método de Newton era dado pela iteração: x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} Consequentemente, em problemas envolvendo sistemas de equações, teremos que o método de Newton será dado pela iteração: x_{n+1} = x_n - J_F^{-1}(x_n) F(x_n) === Problemas de otimização === O método de Newton pode ainda ser visto da seguinte forma: Se F(x)=0, para 0,x\\in \\mathbb{R}^m, podemos definir outra função g(x)=\\frac{\\|F(x)\\|^2}{2}=\\frac{F(x)\\cdot F(x)}{2}, em que \\cdot denota o produto escalar usual. Então o valor mínimo de g(x) é 0, ou seja, \\min_x g(x) =0\\Longleftrightarrow F(x)=0 e a equação F(x)=0 pode ser resolvida como um problema de otimização. Definindo a equação diferencial ordinária \\begin{cases}J_F(u(t))u'(t)&=&-F(u(t))\\\\\\u(0)&=&x;_0\\end{cases}\\qquad\\qquad (*), pode-se mostrar, sob certas condições, que a única solução u(t) dessa equação (*) é tal que \\frac{d\\,g(u(t))}{dt}=-2g(u(t)) . Isso garante que g(u(t)) decresce, enquanto F(u(t)) eq 0, e que g(u(t))= e^{-2t}g(x_0). O uso do método de Euler para determinar uma aproximação para u(t), com tamanho de passo h=1, é equivalente ao método de Newton. Quando desconfiarmos que a matriz jacobiana J_F(x) não possui inversa (em algum ponto), podemos usar a equação diferencial \\begin{cases}v'(t)&=&- abla g(v(t))\\\\\\v(0)&=&x;_0\\end{cases},\\qquad \\qquad (**) em que abla g(v(t))=J_F(v(t))^*F(v(t)), e J_F(v(t))^* denota a matriz transposta da matriz jacobiana J_F(v(t)). Nesse caso, pode-se mostrar que g(v(t)) também é decrescente, enquanto F(v(t)) eq 0; e uso do método de Euler para determinar uma aproximação a solução v(t) (da equação (**)) é equivalente ao método do gradiente. == Exemplos com uma variável == #Neste exemplo, mostraremos porque a função f deve ser diferenciável em xn, para a satisfazer a condição inicial. Considere a função f(x)=|x-3|-1. Essa função possui uma cúspide em (3,-1); portanto, f não é diferenciável nesse ponto. Analisando o gráfico dessa função, percebemos que x=2 e x=4 são suas raízes. Caso iniciemos o método de Newton com x0=3, o processo iterativo falhará porque a derivada de f em x=3 não é definida. #Neste exemplo, mostraremos porque a função f deve ter derivada não nula em xn. Considere a função f(x)=x2-1. Essa função possui uma reta tangente horizontal em (0,-1); portanto, a derivada de f nesse ponto é nula. Como a reta tangente é horizontal, logo ela nunca interceptará o eixo das abcissas e, assim, o método de Newton falhará, pois ocorrerá uma indeterminação matemática (divisão por zero). #Neste exemplo, mostraremos que mesmo escolhendo-se uma aproximação x0 distante da real raiz da função f, o método de Newton ainda assim poderá convergirá rapidamente para a solução de f(x)=0. Considere a função f(x)=sen(x). Se arbitrarmos x0=10,85 rad, valor relativamente distante da primeira raiz, x=π rad, o método convergirá para essa raiz rapidamente. Isso mostra que a primeira aproximação da raiz não necessita ser um valor próximo dela. Existem casos em que essa aproximação é distante da raiz e mesmo assim o método converge, conforme mostrado no exemplo acima. == Considerações sobre o método == O método de Newton é considerado por muitos autores o melhor método para encontrar sucessivas melhores aproximações de raízes (ou zeros) de uma determinada função real e, portanto, tem sido estudado e utilizado em diversos ramos da ciência (Matemática, Física, Engenharia), sendo também muito utilizado na resolução de sistemas não lineares. Além disso, esse método tem sido alvo de novos estudos e aprimoramentos. Em 1984, Allan J. Macleod mostrou, num artigo da International Journal of Mathematical Education in Science and Technology, que o método iterativo de Newton–Raphson para equações não lineares pode ser considerado um membro da família geral de um parâmetro de métodos de segunda ordem. Um ponto importante a ser observado diz respeito a praticidade do método de Newton. Caso a função f seja complicada, encontrar sua derivada pode ser muito trabalhoso e o método torna-se improdutivo. Nesses casos, o método das secantes é mais produtivo de ser utilizado, porque não exige que a derivada de f seja conhecida. ==Ligações externas== *Roots of a function - Rosetta Code - implementações em diversas linguagens de programação *Cálculo Numérico - Um Livro Colaborativo - seção sobre o método de Newton para resolver equações algébricas no livro de Cálculo Numérico mantido pelo Instituto de Matemática e Estatística da Universidade Federal do Rio Grande do Sul. *Cálculo Numérico - Um Livro Colaborativo - seção sobre o método de Newton para resolver sistemas de equações algébricas no livro de Cálculo Numérico mantido pelo Instituto de Matemática e Estatística da Universidade Federal do Rio Grande do Sul. Categoria:Análise numérica Categoria:Algoritmos",
 "title": "Método de Newton–Raphson"
}